{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fairness and Privacy\n",
    "\n",
    "This tutorial demonstrates how to mitigate sex unfairness through demographic parity loss regularization for skin lesion classification on the [ISIC 2019](https://challenge.isic-archive.com/data/#2019) dataset. \n",
    "\n",
    "In this tutorial, we will cover\n",
    "\n",
    "- Prepare the model and the dataset\n",
    "\n",
    "    - Data analysis\n",
    "\n",
    "    - Split the train and test sets\n",
    "\n",
    "    - Define the model\n",
    "\n",
    "- Train and evaluate the model\n",
    "\n",
    "- Unfairness mitigation\n",
    "\n",
    "    - Define demographic parity loss regularization and new training process\n",
    "\n",
    "    - Train and evaluate the model\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "**Note**: To use Google Colab's GPU, click 'Runtime' --> 'Change runtime type' -->  'T4 GPU'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.models import ResNet18_Weights\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "# seed everything\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(0)\n",
    "    torch.cuda.manual_seed_all(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # TODO: If you use Google Colab, uncomment the following code\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/gdrive')\n",
    "\n",
    "# # Change to the path of the tutorial\n",
    "# import os\n",
    "# path = './gdrive/MyDrive/Code/ic/TAIMI_trustworthy' \n",
    "# os.chdir(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Prepare the Skin Lesion Dataset\n",
    "\n",
    "Here we use a small split of the ISIC 2019 dataset (500 samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the data distribution    MEL (melanoma), NV (melanocytic nevus)\n",
    "df = pd.read_csv('Data/ISIC2019/label.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label distribution\n",
    "df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sensitive attribute (sex) distribution\n",
    "df['sex'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ISICDataset(Dataset):\n",
    "    def __init__(self, df, root_dir, transform=None, do_train=False):\n",
    "        self.df = df\n",
    "        self.root_dir = root_dir\n",
    "        self.do_train = do_train\n",
    "        if self.do_train:\n",
    "            self.transform = transform\n",
    "        else:\n",
    "            self.transform = transforms.Compose([\n",
    "                transforms.Resize((224, 224)),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "            ])\n",
    "        print(f'Number of samples: {len(self.df)}')\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        img_name = row['image']\n",
    "        img_path = self.root_dir + '/' + img_name + '.jpg'\n",
    "        image = Image.open(img_path)\n",
    "        label = row['label_code']\n",
    "        sensitive_attr = row['sex_code']\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label, sensitive_attr, img_name\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(size=256, scale=(0.8, 1.0)),\n",
    "    transforms.RandomRotation(degrees=15),\n",
    "    transforms.ColorJitter(),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.CenterCrop(size=224), \n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Data root directory\n",
    "DATA_dir = 'Data/ISIC2019/images'\n",
    "\n",
    "# Create train (80%) and test (20%) datasets\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=2025)\n",
    "train_dataset = ISICDataset(train_df, DATA_dir, transform=train_transform, do_train=True)\n",
    "test_dataset = ISICDataset(test_df, DATA_dir)\n",
    "\n",
    "# Create train and test dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Define the Model\n",
    "\n",
    "Here we utilize a ResNet-18 [1] that has been pre-trained on skin images.\n",
    "\n",
    "[1] He, Kaiming, et al. \"Deep residual learning for image recognition.\" Proceedings of the IEEE conference on computer vision and pattern recognition. 2016."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.resnet18(weights=ResNet18_Weights.IMAGENET1K_V1)\n",
    "model.fc = nn.Linear(512, len(df['label'].unique()))\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Train the Model\n",
    "\n",
    "Define the loss function and optimizer, and train the model on the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "def training(model, criterion, optimizer, train_loader, num_epochs=20):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for images, labels, _, _ in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "    return model\n",
    "\n",
    "# Testing\n",
    "def testing(model, test_loader):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        prediction_list = [] \n",
    "        label_list = []\n",
    "        s_list = []  # Save sensitive attributes\n",
    "        prob_list = []  # Save True class probability for AUC calculation\n",
    "        for images, labels, s, _ in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            probs = F.softmax(outputs.detach(), dim=1)\n",
    "            probs = probs[:, 1]  # Probability of True class\n",
    "\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "            # Save predictions, labels, sensitive attributes, and probabilities of True class\n",
    "            prediction_list.extend(predicted.cpu().numpy())\n",
    "            label_list.extend(labels.cpu().numpy())\n",
    "            s_list.extend(s.cpu().numpy())\n",
    "            prob_list.extend(probs.cpu().numpy())\n",
    "        \n",
    "        print(f'Test Accuracy: {100 * correct / total:.2f}%')\n",
    "    return prediction_list, prob_list, label_list, s_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "# Train the model\n",
    "model = training(model, criterion, optimizer, train_loader, num_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test and save the model\n",
    "predictions, probs, labels, sensitive_attrs = testing(model, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Calculate Evaluation Metrics\n",
    "\n",
    "#### Classification Performance\n",
    "\n",
    "* Accuracy\n",
    "\n",
    "* Area under the curve (AUC)\n",
    "\n",
    "\n",
    "#### Fairness Metrics\n",
    "\n",
    "* Demographic Parity: The predicted positive rate should be equal across sensitive attributes ($s \\in S$).\n",
    "\n",
    "$$\n",
    "DP = \\operatorname{abs}[p(\\hat{y}=1|s=0)-p(\\hat{y}=1|s=1)]\n",
    "$$\n",
    "\n",
    "* Accuracy Parity: The accuracy should be equal across sensitive attributes.\n",
    "\n",
    "$$\n",
    "AP = \\operatorname{abs}[p(\\hat{y}=y|s=0)-p(\\hat{y}=y|s=1)]\n",
    "$$\n",
    "\n",
    "* Equalized Odds: The true positive rates (TPRs) and false positive rates (FPRs) should be equalized across sensitive attributes.\n",
    "\n",
    "$$\n",
    "EOD = \\operatorname{abs}[p(\\hat{y}=1|\\hat{y}=y, s=0)-p(\\hat{y}=1|\\hat{y}=y, s=1)]\n",
    "$$\n",
    "\n",
    "* Equal Opportunity: The the true positive rates should be equalized across sensitive attributes.\n",
    "\n",
    "$$\n",
    "EO = \\operatorname{abs}[p(\\hat{y}=1|\\hat{y}=1, s=j)-p(\\hat{y}=1|\\hat{y}=1, s=j)]\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification performance\n",
    "accuracy = np.mean(np.array(predictions) == np.array(labels))\n",
    "auc = roc_auc_score(labels, probs)\n",
    "print(f'Overall Accuracy: {100*accuracy:.2f}%')\n",
    "print(f'AUC: {auc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fairness performance\n",
    "\n",
    "# Separate predictions, labels by sensitive groups\n",
    "g0_idx = np.where(np.array(sensitive_attrs) == 0)[0]\n",
    "g1_idx = np.where(np.array(sensitive_attrs) == 1)[0]\n",
    "\n",
    "g0_pred = np.array(predictions)[g0_idx]\n",
    "g1_pred = np.array(predictions)[g1_idx]\n",
    "\n",
    "g0_labels = np.array(labels)[g0_idx]\n",
    "g1_labels = np.array(labels)[g1_idx]\n",
    "\n",
    "# Demographic Parity\n",
    "def demographic_parity(g0_pred, g1_pred):\n",
    "    # TODO: Implement Demographic Parity\n",
    "    dp = 0.0\n",
    "    return dp\n",
    "\n",
    "# Accuracy Parity\n",
    "def accuracy_parity(g0_pred, g1_pred, g0_labels, g1_labels):\n",
    "    g0_acc = np.mean(g0_pred==g0_labels)\n",
    "    g1_acc = np.mean(g1_pred==g1_labels)\n",
    "    ap = np.abs(g0_acc - g1_acc)\n",
    "    return ap\n",
    "\n",
    "# Equalized Odds\n",
    "def equalized_odds(g0_pred, g1_pred, g0_labels, g1_labels):\n",
    "    # If no correct predictions, return 0\n",
    "    if np.sum(g0_pred==g0_labels) == 0:\n",
    "        g0_rate = 0\n",
    "    else:\n",
    "        g0_rate = np.sum((g0_pred==1) & (g0_labels==1)) / np.sum(g0_pred==g0_labels)\n",
    "    if np.sum(g1_pred==g1_labels) == 0:\n",
    "        g1_rate = 0\n",
    "    else:\n",
    "        g1_rate = np.sum((g1_pred==1) & (g1_labels==1)) / np.sum(g1_pred==g1_labels)\n",
    "    eod = np.abs(g0_rate - g1_rate)\n",
    "    return eod\n",
    "\n",
    "# Equal Opportunity\n",
    "def equal_opportunity(g0_pred, g1_pred, g0_labels, g1_labels):\n",
    "    g0_tpr = np.sum((g0_pred==1) & (g0_labels==1)) / np.sum(g0_labels==1)\n",
    "    g1_tpr = np.sum((g1_pred==1) & (g1_labels==1)) / np.sum(g1_labels==1)\n",
    "    eo = np.abs(g0_tpr - g1_tpr)\n",
    "    return eo\n",
    "\n",
    "\n",
    "dp = demographic_parity(g0_pred, g1_pred)\n",
    "ap = accuracy_parity(g0_pred, g1_pred, g0_labels, g1_labels)\n",
    "eod = equalized_odds(g0_pred, g1_pred, g0_labels, g1_labels)\n",
    "eo = equal_opportunity(g0_pred, g1_pred, g0_labels, g1_labels)\n",
    "\n",
    "print(f'Demographic Parity: {dp:.4f}')\n",
    "print(f'Accuracy Parity: {ap:.4f}')\n",
    "print(f'Equalized Odds: {eod:.4f}')\n",
    "print(f'Equal Opportunity: {eo:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the model and metrics to avoid confusion in the next section\n",
    "del model, criterion, optimizer\n",
    "del predictions, probs, labels, sensitive_attrs, g0_pred, g1_pred, g0_labels, g1_labels, g0_idx, g1_idx, dp, ap, eod, eo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Mitigate Unfairness Using Demographic Parity Loss\n",
    "\n",
    "Add a demographic parity loss regularization to the original loss. For each training batch, the modified loss is as follows.\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{new} = \\mathcal{L} + w\\mathcal{L}_{dp}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{dp} = \\operatorname{abs}[p(\\hat{y}=1|s=0)-p(\\hat{y}=1|s=1)]\n",
    "$$\n",
    "\n",
    "$w$ is a weight factor to control the magnitude of the demographic parity loss regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demographic_parity_loss(y_pred, sensitive_attr):\n",
    "    \"\"\"\n",
    "    Calculate the demographic parity loss as the difference between\n",
    "    the mean predicted positive rates for different sensitive groups.\n",
    "    \"\"\"\n",
    "    # Group 0 and Group 1 masks\n",
    "    group_0_mask = (sensitive_attr == 0)\n",
    "    group_1_mask = (sensitive_attr == 1)\n",
    "    \n",
    "    group_0_rate = y_pred[group_0_mask].float().mean()\n",
    "    group_1_rate = y_pred[group_1_mask].float().mean()\n",
    "    \n",
    "    dp_loss = torch.abs(group_0_rate - group_1_rate)\n",
    "    return dp_loss\n",
    "\n",
    "# Training with regularization\n",
    "def training_with_dp_regularization(model, criterion, optimizer, train_loader, num_epochs=10, dp_weight=0.1):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for images, labels, sensitive_attr, _ in train_loader:\n",
    "            images, labels, sensitive_attr = images.to(device), labels.to(device), sensitive_attr.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            loss_ce = criterion(outputs, labels)\n",
    "\n",
    "            # Demographic Parity loss   \n",
    "            with torch.no_grad():\n",
    "                _, y_pred = torch.max(outputs.data, 1)\n",
    "                dp_loss = demographic_parity_loss(y_pred, sensitive_attr)\n",
    "            \n",
    "            loss = loss_ce + dp_loss * dp_weight\n",
    "\n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "model = models.resnet18(weights=ResNet18_Weights.IMAGENET1K_V1)\n",
    "model.fc = nn.Linear(512, len(df['label'].unique()))\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "# Use the resampled dataloader to train the model\n",
    "model = training_with_dp_regularization(model, criterion, optimizer, train_loader, num_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "predictions, probs, labels, sensitive_attrs = testing(model, test_loader)\n",
    "\n",
    "# Classification performance\n",
    "auc = roc_auc_score(labels, probs)\n",
    "print(f'AUC: {auc:.4f}')\n",
    "\n",
    "# Fairness performance\n",
    "# Separate predictions, labels by sensitive groups\n",
    "g0_idx = np.where(np.array(sensitive_attrs) == 0)[0]\n",
    "g1_idx = np.where(np.array(sensitive_attrs) == 1)[0]\n",
    "g0_pred = np.array(predictions)[g0_idx]\n",
    "g1_pred = np.array(predictions)[g1_idx]\n",
    "g0_labels = np.array(labels)[g0_idx]\n",
    "g1_labels = np.array(labels)[g1_idx]\n",
    "\n",
    "dp = demographic_parity(g0_pred, g1_pred)\n",
    "ap = accuracy_parity(g0_pred, g1_pred, g0_labels, g1_labels)\n",
    "eod = equalized_odds(g0_pred, g1_pred, g0_labels, g1_labels)\n",
    "eo = equal_opportunity(g0_pred, g1_pred, g0_labels, g1_labels)\n",
    "\n",
    "print(f'Demographic Parity: {dp:.4f}')\n",
    "print(f'Accuracy Parity: {ap:.4f}')\n",
    "print(f'Equalized Odds: {eod:.4f}')\n",
    "print(f'Equal Opportunity: {eo:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Thinking\n",
    "\n",
    "1. Try to adjust the magnitude of the demograhpic parity loss regularization. Can fairness and accuracy always coexist?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trustworthy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
