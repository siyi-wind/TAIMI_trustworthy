{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fairness and Privacy\n",
    "\n",
    "This tutorial demonstrates how to mitigate sex unfairness through demographic parity loss regularization for skin lesion classification on the [ISIC 2019](https://challenge.isic-archive.com/data/#2019) dataset. \n",
    "\n",
    "In this tutorial, we will cover\n",
    "\n",
    "- Prepare the model and the dataset\n",
    "\n",
    "    - Data analysis\n",
    "\n",
    "    - Split the train and test sets\n",
    "\n",
    "    - Define the model\n",
    "\n",
    "- Train and evaluate the model\n",
    "\n",
    "- Unfairness mitigation\n",
    "\n",
    "    - Define demographic parity loss regularization and new training process\n",
    "\n",
    "    - Train and evaluate the model\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "**Note**: To use Google Colab's GPU, click 'Runtime' --> 'Change runtime type' -->  'T4 GPU'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.models import ResNet18_Weights\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "# seed everything\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(0)\n",
    "    torch.cuda.manual_seed_all(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # TODO If you use Google Colab, uncomment the following code\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/gdrive')\n",
    "\n",
    "# # Change to the path of the tutorial\n",
    "# import os\n",
    "# path = './gdrive/MyDrive/Code/ic/TAIMI_trustworthy' \n",
    "# os.chdir(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Prepare the Skin Lesion Dataset\n",
    "\n",
    "Here we use a small split of the ISIC 2019 dataset (500 samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>label</th>\n",
       "      <th>label_code</th>\n",
       "      <th>sex</th>\n",
       "      <th>sex_code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ISIC_0034321</td>\n",
       "      <td>NV</td>\n",
       "      <td>0</td>\n",
       "      <td>female</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ISIC_0034322</td>\n",
       "      <td>NV</td>\n",
       "      <td>0</td>\n",
       "      <td>male</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ISIC_0034324</td>\n",
       "      <td>NV</td>\n",
       "      <td>0</td>\n",
       "      <td>male</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ISIC_0034325</td>\n",
       "      <td>NV</td>\n",
       "      <td>0</td>\n",
       "      <td>female</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ISIC_0034328</td>\n",
       "      <td>NV</td>\n",
       "      <td>0</td>\n",
       "      <td>male</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>ISIC_0054574</td>\n",
       "      <td>MEL</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>ISIC_0054577</td>\n",
       "      <td>MEL</td>\n",
       "      <td>1</td>\n",
       "      <td>male</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>ISIC_0054612</td>\n",
       "      <td>MEL</td>\n",
       "      <td>1</td>\n",
       "      <td>male</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>ISIC_0054637</td>\n",
       "      <td>MEL</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>ISIC_0054645</td>\n",
       "      <td>MEL</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            image label  label_code     sex  sex_code\n",
       "0    ISIC_0034321    NV           0  female         0\n",
       "1    ISIC_0034322    NV           0    male         1\n",
       "2    ISIC_0034324    NV           0    male         1\n",
       "3    ISIC_0034325    NV           0  female         0\n",
       "4    ISIC_0034328    NV           0    male         1\n",
       "..            ...   ...         ...     ...       ...\n",
       "495  ISIC_0054574   MEL           1  female         0\n",
       "496  ISIC_0054577   MEL           1    male         1\n",
       "497  ISIC_0054612   MEL           1    male         1\n",
       "498  ISIC_0054637   MEL           1  female         0\n",
       "499  ISIC_0054645   MEL           1  female         0\n",
       "\n",
       "[500 rows x 5 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Visualize the data distribution    MEL (melanoma), NV (melanocytic nevus)\n",
    "df = pd.read_csv('Data/ISIC2019/label.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "NV     250\n",
       "MEL    250\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Label distribution\n",
    "df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sex\n",
       "male      268\n",
       "female    232\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sensitive attribute (sex) distribution\n",
    "df['sex'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 400\n",
      "Number of samples: 100\n"
     ]
    }
   ],
   "source": [
    "class ISICDataset(Dataset):\n",
    "    def __init__(self, df, root_dir, transform=None, do_train=False):\n",
    "        self.df = df\n",
    "        self.root_dir = root_dir\n",
    "        self.do_train = do_train\n",
    "        if self.do_train:\n",
    "            self.transform = transform\n",
    "        else:\n",
    "            self.transform = transforms.Compose([\n",
    "                transforms.Resize((224, 224)),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "            ])\n",
    "        print(f'Number of samples: {len(self.df)}')\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        img_name = row['image']\n",
    "        img_path = self.root_dir + '/' + img_name + '.jpg'\n",
    "        image = Image.open(img_path)\n",
    "        label = row['label_code']\n",
    "        sensitive_attr = row['sex_code']\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label, sensitive_attr, img_name\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(size=256, scale=(0.8, 1.0)),\n",
    "    transforms.RandomRotation(degrees=15),\n",
    "    transforms.ColorJitter(),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.CenterCrop(size=224), \n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Data root directory\n",
    "DATA_dir = 'Data/ISIC2019/images'\n",
    "\n",
    "# Create train (80%) and test (20%) datasets\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=2025)\n",
    "train_dataset = ISICDataset(train_df, DATA_dir, transform=train_transform, do_train=True)\n",
    "test_dataset = ISICDataset(test_df, DATA_dir)\n",
    "\n",
    "# Create train and test dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Define the Model\n",
    "\n",
    "Here we utilize a ResNet-18 [1] that has been pre-trained on skin images.\n",
    "\n",
    "[1] He, Kaiming, et al. \"Deep residual learning for image recognition.\" Proceedings of the IEEE conference on computer vision and pattern recognition. 2016."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.resnet18(weights=ResNet18_Weights.IMAGENET1K_V1)\n",
    "model.fc = nn.Linear(512, len(df['label'].unique()))\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Train the Model\n",
    "\n",
    "Define the loss function and optimizer, and train the model on the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "def training(model, criterion, optimizer, train_loader, num_epochs=20):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for images, labels, _, _ in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "    return model\n",
    "\n",
    "# Testing\n",
    "def testing(model, test_loader):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        prediction_list = [] \n",
    "        label_list = []\n",
    "        s_list = []  # Save sensitive attributes\n",
    "        prob_list = []  # Save True class probability for AUC calculation\n",
    "        for images, labels, s, _ in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            probs = F.softmax(outputs.detach(), dim=1)\n",
    "            probs = probs[:, 1]  # Probability of True class\n",
    "\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "            # Save predictions, labels, sensitive attributes, and probabilities of True class\n",
    "            prediction_list.extend(predicted.cpu().numpy())\n",
    "            label_list.extend(labels.cpu().numpy())\n",
    "            s_list.extend(s.cpu().numpy())\n",
    "            prob_list.extend(probs.cpu().numpy())\n",
    "        \n",
    "        print(f'Test Accuracy: {100 * correct / total:.2f}%')\n",
    "    return prediction_list, prob_list, label_list, s_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 0.2964\n",
      "Epoch [2/10], Loss: 0.3992\n",
      "Epoch [3/10], Loss: 0.0312\n",
      "Epoch [4/10], Loss: 0.5032\n",
      "Epoch [5/10], Loss: 0.1196\n",
      "Epoch [6/10], Loss: 0.0379\n",
      "Epoch [7/10], Loss: 0.2375\n",
      "Epoch [8/10], Loss: 0.0238\n",
      "Epoch [9/10], Loss: 0.1349\n",
      "Epoch [10/10], Loss: 0.1185\n"
     ]
    }
   ],
   "source": [
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "# Train the model\n",
    "model = training(model, criterion, optimizer, train_loader, num_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 91.00%\n"
     ]
    }
   ],
   "source": [
    "# Test and save the model\n",
    "predictions, probs, labels, sensitive_attrs = testing(model, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Calculate Evaluation Metrics\n",
    "\n",
    "#### Classification Performance\n",
    "\n",
    "* Accuracy\n",
    "\n",
    "* Area under the curve (AUC)\n",
    "\n",
    "\n",
    "#### Fairness Metrics\n",
    "\n",
    "* Demographic Parity: The predicted positive rate should be equal across sensitive attributes ($s \\in S$).\n",
    "\n",
    "$$\n",
    "DP = \\operatorname{abs}[p(\\hat{y}=1|s=0)-p(\\hat{y}=1|s=1)]\n",
    "$$\n",
    "\n",
    "* Accuracy Parity: The accuracy should be equal across sensitive attributes.\n",
    "\n",
    "$$\n",
    "AP = \\operatorname{abs}[p(\\hat{y}=y|s=0)-p(\\hat{y}=y|s=1)]\n",
    "$$\n",
    "\n",
    "* Equalized Odds: The true positive rates (TPRs) and false positive rates (FPRs) should be equalized across sensitive attributes.\n",
    "\n",
    "$$\n",
    "EOD = \\operatorname{abs}[p(\\hat{y}=1|\\hat{y}=y, s=0)-p(\\hat{y}=1|\\hat{y}=y, s=1)]\n",
    "$$\n",
    "\n",
    "* Equal Opportunity: The the true positive rates should be equalized across sensitive attributes.\n",
    "\n",
    "$$\n",
    "EO = \\operatorname{abs}[p(\\hat{y}=1|\\hat{y}=1, s=j)-p(\\hat{y}=1|\\hat{y}=1, s=j)]\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Accuracy: 91.00%\n",
      "AUC: 0.9700\n"
     ]
    }
   ],
   "source": [
    "# Classification performance\n",
    "accuracy = np.mean(np.array(predictions) == np.array(labels))\n",
    "auc = roc_auc_score(labels, probs)\n",
    "print(f'Overall Accuracy: {100*accuracy:.2f}%')\n",
    "print(f'AUC: {auc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Demographic Parity: 0.1196\n",
      "Accuracy Parity: 0.1036\n",
      "Equalized Odds: 0.0986\n",
      "Equal Opportunity: 0.1739\n"
     ]
    }
   ],
   "source": [
    "# Fairness performance\n",
    "\n",
    "# Separate predictions, labels by sensitive groups\n",
    "g0_idx = np.where(np.array(sensitive_attrs) == 0)[0]\n",
    "g1_idx = np.where(np.array(sensitive_attrs) == 1)[0]\n",
    "\n",
    "g0_pred = np.array(predictions)[g0_idx]\n",
    "g1_pred = np.array(predictions)[g1_idx]\n",
    "\n",
    "g0_labels = np.array(labels)[g0_idx]\n",
    "g1_labels = np.array(labels)[g1_idx]\n",
    "\n",
    "# Demographic Parity\n",
    "def demographic_parity(g0_pred, g1_pred):\n",
    "    dp = np.abs(np.mean(g0_pred==1) - np.mean(g1_pred==1))\n",
    "    return dp\n",
    "\n",
    "# Accuracy Parity\n",
    "def accuracy_parity(g0_pred, g1_pred, g0_labels, g1_labels):\n",
    "    g0_acc = np.mean(g0_pred==g0_labels)\n",
    "    g1_acc = np.mean(g1_pred==g1_labels)\n",
    "    ap = np.abs(g0_acc - g1_acc)\n",
    "    return ap\n",
    "\n",
    "# Equalized Odds\n",
    "def equalized_odds(g0_pred, g1_pred, g0_labels, g1_labels):\n",
    "    # If no correct predictions, return 0\n",
    "    if np.sum(g0_pred==g0_labels) == 0:\n",
    "        g0_rate = 0\n",
    "    else:\n",
    "        g0_rate = np.sum((g0_pred==1) & (g0_labels==1)) / np.sum(g0_pred==g0_labels)\n",
    "    if np.sum(g1_pred==g1_labels) == 0:\n",
    "        g1_rate = 0\n",
    "    else:\n",
    "        g1_rate = np.sum((g1_pred==1) & (g1_labels==1)) / np.sum(g1_pred==g1_labels)\n",
    "    eod = np.abs(g0_rate - g1_rate)\n",
    "    return eod\n",
    "\n",
    "# Equal Opportunity\n",
    "def equal_opportunity(g0_pred, g1_pred, g0_labels, g1_labels):\n",
    "    g0_tpr = np.sum((g0_pred==1) & (g0_labels==1)) / np.sum(g0_labels==1)\n",
    "    g1_tpr = np.sum((g1_pred==1) & (g1_labels==1)) / np.sum(g1_labels==1)\n",
    "    eo = np.abs(g0_tpr - g1_tpr)\n",
    "    return eo\n",
    "\n",
    "\n",
    "dp = demographic_parity(g0_pred, g1_pred)\n",
    "ap = accuracy_parity(g0_pred, g1_pred, g0_labels, g1_labels)\n",
    "eod = equalized_odds(g0_pred, g1_pred, g0_labels, g1_labels)\n",
    "eo = equal_opportunity(g0_pred, g1_pred, g0_labels, g1_labels)\n",
    "\n",
    "print(f'Demographic Parity: {dp:.4f}')\n",
    "print(f'Accuracy Parity: {ap:.4f}')\n",
    "print(f'Equalized Odds: {eod:.4f}')\n",
    "print(f'Equal Opportunity: {eo:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the model and metrics to avoid confusion in the next section\n",
    "del model, criterion, optimizer\n",
    "del predictions, probs, labels, sensitive_attrs, g0_pred, g1_pred, g0_labels, g1_labels, g0_idx, g1_idx, dp, ap, eod, eo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Mitigate Unfairness Using Demographic Parity Loss\n",
    "\n",
    "Add a demographic parity loss regularization to the original loss. For each training batch, the modified loss is as follows.\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{new} = \\mathcal{L} + \\mathcal{L}_{dp}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{dp} = \\operatorname{abs}[p(\\hat{y}=1|s=0)-p(\\hat{y}=1|s=1)]\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demographic_parity_loss(y_pred, sensitive_attr):\n",
    "    \"\"\"\n",
    "    Calculate the demographic parity loss as the difference between\n",
    "    the mean predicted positive rates for different sensitive groups.\n",
    "    \"\"\"\n",
    "    # Group 0 and Group 1 masks\n",
    "    group_0_mask = (sensitive_attr == 0)\n",
    "    group_1_mask = (sensitive_attr == 1)\n",
    "    \n",
    "    group_0_rate = y_pred[group_0_mask].float().mean()\n",
    "    group_1_rate = y_pred[group_1_mask].float().mean()\n",
    "    \n",
    "    dp_loss = torch.abs(group_0_rate - group_1_rate)\n",
    "    return dp_loss\n",
    "\n",
    "# Training with regularization\n",
    "def training_with_dp_regularization(model, criterion, optimizer, train_loader, num_epochs=10, dp_weight=0.1):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for images, labels, sensitive_attr, _ in train_loader:\n",
    "            images, labels, sensitive_attr = images.to(device), labels.to(device), sensitive_attr.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            loss_ce = criterion(outputs, labels)\n",
    "\n",
    "            # Demographic Parity loss   \n",
    "            with torch.no_grad():\n",
    "                _, y_pred = torch.max(outputs.data, 1)\n",
    "                dp_loss = demographic_parity_loss(y_pred, sensitive_attr)\n",
    "            \n",
    "            loss = loss_ce + dp_loss * dp_weight\n",
    "\n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 0.4710\n",
      "Epoch [2/10], Loss: 0.5750\n",
      "Epoch [3/10], Loss: 0.2004\n",
      "Epoch [4/10], Loss: 0.4081\n",
      "Epoch [5/10], Loss: 0.1531\n",
      "Epoch [6/10], Loss: 0.2015\n",
      "Epoch [7/10], Loss: 0.0213\n",
      "Epoch [8/10], Loss: 0.2654\n",
      "Epoch [9/10], Loss: 0.0230\n",
      "Epoch [10/10], Loss: 0.0427\n"
     ]
    }
   ],
   "source": [
    "# Define the model\n",
    "model = models.resnet18(weights=ResNet18_Weights.IMAGENET1K_V1)\n",
    "model.fc = nn.Linear(512, len(df['label'].unique()))\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "# Use the resampled dataloader to train the model\n",
    "model = training_with_dp_regularization(model, criterion, optimizer, train_loader, num_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 96.00%\n",
      "AUC: 0.9916\n",
      "Demographic Parity: 0.1000\n",
      "Accuracy Parity: 0.0416\n",
      "Equalized Odds: 0.0835\n",
      "Equal Opportunity: 0.0870\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "predictions, probs, labels, sensitive_attrs = testing(model, test_loader)\n",
    "\n",
    "# Classification performance\n",
    "auc = roc_auc_score(labels, probs)\n",
    "print(f'AUC: {auc:.4f}')\n",
    "\n",
    "# Fairness performance\n",
    "# Separate predictions, labels by sensitive groups\n",
    "g0_idx = np.where(np.array(sensitive_attrs) == 0)[0]\n",
    "g1_idx = np.where(np.array(sensitive_attrs) == 1)[0]\n",
    "g0_pred = np.array(predictions)[g0_idx]\n",
    "g1_pred = np.array(predictions)[g1_idx]\n",
    "g0_labels = np.array(labels)[g0_idx]\n",
    "g1_labels = np.array(labels)[g1_idx]\n",
    "\n",
    "dp = demographic_parity(g0_pred, g1_pred)\n",
    "ap = accuracy_parity(g0_pred, g1_pred, g0_labels, g1_labels)\n",
    "eod = equalized_odds(g0_pred, g1_pred, g0_labels, g1_labels)\n",
    "eo = equal_opportunity(g0_pred, g1_pred, g0_labels, g1_labels)\n",
    "\n",
    "print(f'Demographic Parity: {dp:.4f}')\n",
    "print(f'Accuracy Parity: {ap:.4f}')\n",
    "print(f'Equalized Odds: {eod:.4f}')\n",
    "print(f'Equal Opportunity: {eo:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trustworthy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
